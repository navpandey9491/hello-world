---
title: "R Notebook"
output: html_notebook
---
# CST4070 - Submmitive Assignment

## Pre-processing

#### Importing the Library for reading and manipulating csv file
```{r}
library(readr)
library(dplyr) 
```
#### Importing the datasets
```{r}
bike_j <- read_csv('bike_journeys.csv') 
bike_s <- read_csv('bike_stations.csv')
london_c <- read_csv('London_census.csv')
```
#### First looking at the data to understand the 
```{r}
head(bike_j)
```
```{r}
head(bike_s)
```
```{r}
head(london_c)
```
#### Inspecting the data for any missing value using missmap library
#### Checking whether data contains null values or outliers.
#### The Amelia package is useful to see whether our data contains missing values, here we can see that there is no missing value.
```{r}
library(Amelia) 
missmap(london_c)

```
```{r}
library(Amelia) 
missmap(bike_s)
```
```{r}
library(Amelia) 
missmap(bike_j)
```
#### Checking the consistency and uniqueness between the data set.
```{r}
length(unique(bike_j$Start_Station_ID))
```
```{r}
length(unique(bike_s$Station_ID))
```
#### We can see that both datasets contain the field Station_ID and Start_Startion_ID which represents station and interset function is used here to get the unique values of station_ID
```{r}
length(unique(intersect(bike_j$Start_Station_ID,bike_s$Station_ID)))
```
#### We can see above that we have the dataset have 771 unique values, which is a good overlap

## Hypothesis

#### H1. Bike rented from a bike station will be higher where ratio of number of employees are higher near that station

#### H2. Bike rented will be higher in a bike station which is close proximity of the green space such as parks and ground.

#### H3. Higher the number of bike rented in sation where ratio of number of people born in UK are more.

#### H4. Bike rented in a bike station will be higher where density of population is higher near that station

#### H5. Bike rented in a bike station will be higher during the month of summer season compared to winter season.

## Metrics

#### To prove/disprove our hypothesis, we propose the following metrics:

#### To tackle H1 we will use NoEmployee data from London_census to calculate ratio of employee using this formula RatioEmployee = RatioEmployee = (NoEmployee / (PopDen * AreaSqKm))

#### To tackle H2 we will use GrenSpace from LondonCensus data where we will consider the greenspace in square kilo meter.

#### To tackle H3 we will use BornUK and NotBornUK from London census data to calculate ratio as RatioBornUK = (BornUK / (BornUK + NotBornUK))

#### To tackle H4 we will use Station_ID from bike_station data and population density from London_census data.

#### To prove hypothesis H5 we will use Start_month from Bike_journey data.


#### Removing unnecessary columns from london census dataset and used row wise manipulation and transforming the data as per our hypothesis.

## Data Processing

#### We need to transform our data from format, WardCode, WardName, borough, NESW, AreaSqKm, lon, lat, IncomeScor, LivingEnSc, NoEmployee, GrenSpace, PopDen, BornUK, NotBornUK, NoCTFtoH, NoDwelling, NoFlats, NoHouses, NoOwndDwel, MedHPrice into the format below#####

#### <RatioEmployee, RatioBornUK, start_month, greenspace, bike_station> Here the data is transformed and unnecessary columns are eliminated from the dataset.

#### Forward-pipe operator is used here to pass the left-hand side input through the right-hand side operator to do the computation.
```{r}
london_census <- london_c %>% select(-WardCode, -WardName, -borough, -NESW)%>% rowwise() %>%      transform(RatioEmployee = (NoEmployee / (PopDen * AreaSqKm)), # Metrics     
                                                                                                            RatioBornUK = (BornUK / (BornUK + NotBornUK))) %>%  select(-NoEmployee, -PopDen, -BornUK, -NotBornUK) # Remove unnecessary columns 

head(london_census)
```
#### Now the next step is to merge the dataset together, have used **geosphere package**. We were given lat and log of stations and we are given centroid of the ward in London with its area in sq KM. We calulated the distance from the station to the centroid using distgeo function and if the distance is less than the radius of the ward we implicitly attached the station id to that ward.

#### The distGeo function calculating the distance between the 2 pairs of coordinates. As long a the vectors of the coordinates are of the same length or an even multiple of each other it will still work. Since there is not a 1 to 1 relationship between **bike_s** and **london_census**, merging all of the data into 1 large dataframe called station_detail to perform the calculation.

```{r}
library(geosphere) # For working with geospatial data 
station_detail <- merge(bike_s, london_census, all = TRUE) %>% # Merge two data frames     
  rowwise() %>% filter(distGeo(c(Longitude, Latitude), c(lon, lat)) <= (sqrt(AreaSqKm / pi) * 1000)) # Check distance between station and centroid of ward, if less than radius of ward include record 
head(station_detail)
```
#### Here, selected the necessary column from the **bike_j** dataset like Start_Station_ID, Start_Hour, Start_Month, Start_Date for computation and renamed the column Start_Station_ID to Station_ID. The new columns are stored in **bike_journey dataset** so that we can join the next step.
```{r}
bike_journey <- bike_j %>% select(Start_Station_ID, Start_Hour, Start_Month, Start_Date) %>% rename(Station_ID = Start_Station_ID) # Rename column so that we can join in next step 

head(bike_journey)
```
#### Unique and common values from bike_journey and station_detail is extracted using station_ID column, the output has a good and unique overlap .
```{r}
length(unique(intersect(bike_journey$Station_ID,station_detail$Station_ID)))
```
#### Joined **bike_journey** and **station_detail** dataset using common station_ID in both the datasets with the help of inner_join
#### Used grouped by start month, start date and start hour with station ID, so that we got the rides taken in particular station with per hour granularity. Also, removed unnecessary columns which are not mandatory during computation.
#### info is the final merged dataset on which operation is going to be perfomed.
```{r}
info <- inner_join(bike_journey, station_detail) %>% # Join on Station_ID     
  count(name = 'Rides', Station_ID, Start_Month, Start_Date, Start_Hour) %>% #      
  inner_join(station_detail) %>% # Add remaning columns     
  select(-Station_Name, -lon, -lat) %>% # Removed unnecessary columns     
  select(Rides, everything()) # Putting the Rides column first

head(info)
```
#### Showing the number of rides taken from station per hour. For time granularity, I have selected records per hour.
```{r}
info_hour_zero <- info %>%filter(Start_Hour == 0) # For time granularity, we are selecting records per hour 
head(info_hour_zero)

```
#### Showing the number of rides taken from station at Hour 1 AM. For time granularity, I have selected records per hour
```{r}
info_hour_one <- info %>% filter(Start_Hour == 1) # For time granularity, we are selecting records per hour 
head(info_hour_one)
```
#### Showing the number of rides taken from stations at 11 PM.
```{r}
info_hour_23 <- info %>%     filter(Start_Hour == 23) # For time granularity, we are selecting records per hour 
head(info_hour_23)
```
#### Below we have checked whether the data contains correlation or not and we can see that there is very less correlation is present.
```{r}
library(corrplot) # For working with correlation plots 
corrplot(cor(info)) 
```
#### Now for implementing the algorithm we have used train and test method which is quick solution
```{r}
# Training & Testing
set.seed(100) 
train_data <- sample(1:nrow(info_hour_zero), 0.85*nrow(info_hour_zero)) # Select 85 percent for training data
train <- info_hour_zero[train_data,]# Segregated training data 
test <- info_hour_zero[-train_data,] # Segregated test data 
  head(train)
```
```{r}
head(test)
```
#### Applying linear regression on the given dataset for training the algorithm and then the trained model is tested for prediction
```{r}
linear_regression <- lm(Rides ~ ., data=train) # Use linear regression on training data 
train_predict = predict(linear_regression, train) # Check prediction
```
```{r}
test_predict = predict(linear_regression, test)
```
#### We can see that both the R2 are very similar, which indicates the stability of the model. Therefore we do not need any regularisation.
```{r}
print(paste('R2 on train data:', cor(train_predict, train$Rides)^2))
print(paste('R2 on test data:', cor(test_predict, test$Rides)^2))
```
```{r}
summary(linear_regression)
```
Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
